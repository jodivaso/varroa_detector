{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc722f23",
   "metadata": {},
   "source": [
    "# Refinement step\n",
    "\n",
    "Once the model is trained, this Jupyter notebook includes the code to perform the refinement step to obtain better bounding boxes. It also includes some benchmarking about the performance of the best models with and without the refinement step and deblurGAN.\n",
    "\n",
    "Download the models from: https://zenodo.org/records/10231845/files/models.zip?download=1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65613947",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ba49d2f-741d-4478-b494-d7abe92fdb1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import os, random, time\n",
    "import cv2 as cv2\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision import io\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection import FasterRCNN, fasterrcnn_resnet50_fpn_v2, fasterrcnn_resnet50_fpn, fasterrcnn_mobilenet_v3_large_320_fpn\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "import torchvision.transforms.functional as F_vision\n",
    "from torchvision.utils import make_grid\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.data.sampler import SequentialSampler\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import albumentations.pytorch\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import rawpy\n",
    "import imageio\n",
    "import math\n",
    "from pathlib import Path\n",
    "from os.path import isfile, join\n",
    "from torchvision.models.detection.backbone_utils import resnet_fpn_backbone"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd0bd40",
   "metadata": {},
   "source": [
    "## Parameters\n",
    "\n",
    "Here one should select the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5de49e6-adc2-49ec-b786-da400c1e97f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bbox_params = A.BboxParams(format = 'pascal_voc',\n",
    "         min_visibility = 0.6,\n",
    "         label_fields = ['labels'])\n",
    "\n",
    "BOXES_FOLDER = \"dataset/labels/\"\n",
    "\n",
    "# One should adjust the following parameters according to the chosen model. \n",
    "# Ideally, they should have been obtained automatically, but it is not the case.\n",
    "\n",
    "CROP_X, CROP_Y = 224, 224\n",
    "THRESHOLD_CONFIDENCE = 0.50\n",
    "DEBLURGAN = True\n",
    "\n",
    "\n",
    "device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(device)\n",
    "\n",
    "# Choose the corresponding model:\n",
    "model = fasterrcnn_resnet50_fpn(num_classes=2)\n",
    "# backbone = resnet_fpn_backbone(backbone_name=\"resnet18\", weights=\"DEFAULT\")\n",
    "# model = FasterRCNN(backbone, num_classes=2)   \n",
    "\n",
    "# Load the state of the model:\n",
    "model.to(device)\n",
    "model.load_state_dict(torch.load(\"models/0094_fasterrcnn_resnet50_fpn_deblurGAN/modelo.bin\"))\n",
    "# model.load_state_dict(torch.load(\"models/0070_fasterrcnn_resnet50_fpn_th50/modelo.bin\"))\n",
    "# model.load_state_dict(torch.load(\"models/0105_resnet18_fpn_th50_deblurGAN/modelo_7752.bin\")) \n",
    "# model.load_state_dict(torch.load(\"models/0096_resnet18_fpn_th50/modelo.bin\")) \n",
    "# model.load_state_dict(torch.load(\"models/0103_resnet152_fpn_th50/modelo.bin\")) \n",
    "# model.load_state_dict(torch.load(\"models/0104_resnet152_fpn_th50_deblurGAN/modelo.bin\")) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e02898f-620c-4c6c-b446-13e362d8bd02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To parse annotations\n",
    "import xml.etree.ElementTree as ET\n",
    "import torchvision.transforms.functional as FT\n",
    "\n",
    "# Label map\n",
    "voc_labels = ('varroa', 'pupe')\n",
    "label_map = {k: v + 1 for v, k in enumerate(voc_labels)}\n",
    "label_map['background'] = 0\n",
    "rev_label_map = {v: k for k, v in label_map.items()}  # Inverse mapping\n",
    "\n",
    "def parse_annotation(annotation_path):\n",
    "    tree = ET.parse(annotation_path)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    boxes = list()\n",
    "    labels = list()\n",
    "    difficulties = list()\n",
    "    for object in root.iter('object'):\n",
    "\n",
    "        difficult = int(object.find('difficult').text == '1')\n",
    "\n",
    "        label = object.find('name').text.lower().strip()\n",
    "        if label not in label_map:\n",
    "            continue\n",
    "\n",
    "        bbox = object.find('bndbox')\n",
    "        xmin = int(bbox.find('xmin').text) - 1\n",
    "        ymin = int(bbox.find('ymin').text) - 1\n",
    "        xmax = int(bbox.find('xmax').text) - 1\n",
    "        ymax = int(bbox.find('ymax').text) - 1\n",
    "\n",
    "        boxes.append([xmin, ymin, xmax, ymax])\n",
    "        labels.append(label_map[label])\n",
    "        difficulties.append(difficult)\n",
    "    return {'boxes': boxes, 'labels': labels, 'difficulties': difficulties}\n",
    "\n",
    "\n",
    "def predict_image_batch(images, threshold=THRESHOLD_CONFIDENCE):\n",
    "    output=[]\n",
    "    scores = []\n",
    "    final_boxes = []\n",
    "    with torch.no_grad():\n",
    "        preds = model(images)\n",
    "        for npred, pred_uniq in enumerate(preds):\n",
    "            #preds_prob.append(pred_uniq['scores'].detach().cpu().numpy())\n",
    "            cuales = (pred_uniq['scores']>threshold).detach().cpu().numpy() \n",
    "            boxes = pred_uniq['boxes'].cpu().numpy().astype(np.int32)\n",
    "            img = np.ascontiguousarray(images[npred].permute(1,2,0).detach().cpu().numpy(), dtype=np.float32)\n",
    "            img_cp = img.copy()\n",
    "            num_varroas = np.sum(cuales)\n",
    "            aux_boxes=[]\n",
    "            if num_varroas>0:\n",
    "                for nbox, box in enumerate(boxes[cuales]):\n",
    "                    cv2.rectangle(img,(box[0], box[1]),(box[2], box[3]),(255, 0, 0),2)\n",
    "                    aux_boxes.append((box[0], box[1], box[2], box[3]))\n",
    "                    #fig = plt.figure(figsize=(5, 5))\n",
    "                #plt.imshow(img)\n",
    "            final_boxes.append(aux_boxes)\n",
    "            s = pred_uniq['scores'].cpu().numpy()[cuales]\n",
    "            scores.append(s)\n",
    "            output.append(img*255)\n",
    "    return output, final_boxes, scores\n",
    "\n",
    "\n",
    "\n",
    "def predict_image(input_image_name, model):\n",
    "\n",
    "    model.eval()\n",
    "    preds_prob = []\n",
    "    targets_num = []\n",
    "\n",
    "    valid_imagenes=[]\n",
    "    positions=[] \n",
    "\n",
    "    if input_image_name.endswith(\".DNG\"):\n",
    "        with rawpy.imread(input_image_name) as raw:\n",
    "            input_image = raw.postprocess(use_camera_wb=True, use_auto_wb=False, output_color=rawpy.ColorSpace.sRGB)\n",
    "            input_image = input_image.astype(np.float32)\n",
    "            input_image /= 255.0\n",
    "            input_image = cv2.cvtColor(input_image, cv2.COLOR_RGB2BGR)\n",
    "    else:\n",
    "        input_image = cv2.imread(input_image_name)\n",
    "        input_image = input_image.astype(np.float32)\n",
    "        input_image /= 255.0\n",
    "        input_image = np.array(input_image)\n",
    "\n",
    "    pos_x, pos_y = 0, 0\n",
    "\n",
    "    size_img_y, size_img_x, _ = input_image.shape\n",
    "\n",
    "    image_orig = input_image\n",
    "    boxes_orig = []\n",
    "    labels = torch.ones(0, dtype=torch.int64)\n",
    "    while True:\n",
    "        x_max = pos_x+CROP_X\n",
    "        y_max = pos_y+CROP_Y\n",
    "        if x_max>=size_img_x: \n",
    "            x_max = size_img_x\n",
    "        if y_max>=size_img_y:\n",
    "            y_max = size_img_y\n",
    "\n",
    "        transform = A.Compose([A.Crop(x_min = pos_x, y_min=pos_y, x_max=x_max, y_max=y_max),\n",
    "                               ToTensorV2()],\n",
    "                               bbox_params=bbox_params)\n",
    "        result_transform = transform(image=image_orig, bboxes=boxes_orig, labels=labels)\n",
    "        #result_transform = transform(image=image_orig)\n",
    "        valid_imagenes.append(result_transform['image'])\n",
    "        positions.append((pos_x, pos_y))\n",
    "        pos_x += CROP_X\n",
    "        if pos_x>=size_img_x:\n",
    "            pos_x = 0\n",
    "            pos_y += CROP_Y\n",
    "            # Sal\n",
    "            if pos_y >= size_img_y:\n",
    "                break\n",
    "\n",
    "    # Number of batches. It should be a divisor of the total number of images and as smaller as possible (to \n",
    "    # use as much VRAM as possible)\n",
    "    n_batch = 18 \n",
    "    parte = len(valid_imagenes)//n_batch # This would be the batch size\n",
    "    trozos=[]\n",
    "    final_boxes = []\n",
    "    scores = []\n",
    "    for t in range(n_batch):\n",
    "        images = list(image.to(device) for image in valid_imagenes[t*parte:(t+1)*parte])\n",
    "        r, bb, s = predict_image_batch(images, threshold=THRESHOLD_CONFIDENCE)\n",
    "        final_boxes.extend(bb)\n",
    "        scores.extend(s)\n",
    "        trozos+=r\n",
    "\n",
    "    bboxes = []\n",
    "\n",
    "    for ((x,y), b) in zip(positions, final_boxes):\n",
    "        if b == []: \n",
    "            bboxes.append([])\n",
    "        else:\n",
    "            aux_boxes = []\n",
    "\n",
    "            for (x_min, y_min, x_max, y_max) in b:\n",
    "                aux_boxes.append((x_min + x , y_min + y,  x_max + x, y_max + y))\n",
    "            bboxes.append(aux_boxes)\n",
    "\n",
    "    resultado=[]\n",
    "\n",
    "    nrows = math.ceil(size_img_y / CROP_Y)\n",
    "    ncols = math.ceil(size_img_x / CROP_X)\n",
    "\n",
    "    i=0\n",
    "    for row in range(nrows):\n",
    "        indices = [i + j for j in range(ncols)]\n",
    "        imagenes_fila = [trozos[i] for i in indices]\n",
    "        i=(row+1)*ncols\n",
    "        imagen_final_fila = cv2.hconcat(imagenes_fila)\n",
    "        resultado.append(imagen_final_fila)\n",
    "\n",
    "    resultado = cv2.vconcat(resultado)\n",
    "    name = Path(input_image_name).stem \n",
    "\n",
    "    return positions, bboxes, scores\n",
    "\n",
    "\n",
    "def predict_small_image(input_image, model):\n",
    "    model.eval()\n",
    "    transform = A.Compose([ToTensorV2()])\n",
    "    result_transform = transform(image=input_image)\n",
    "    images = []\n",
    "    images.append(result_transform['image'])\n",
    "    image = list(image.to(device) for image in images)\n",
    "    with torch.no_grad():\n",
    "        preds = model(image)\n",
    "        for npred, pred_uniq in enumerate(preds):\n",
    "            cuales = (pred_uniq['scores']>0.50).detach().cpu().numpy() \n",
    "            boxes = pred_uniq['boxes'].cpu().numpy().astype(np.int32)\n",
    "            scores = pred_uniq['scores'].cpu().numpy()\n",
    "            return boxes[cuales], scores[cuales]\n",
    "\n",
    "\n",
    "def bounding_boxes_intersection_area (bb1, bb2):\n",
    "  # determine the (x, y)-coordinates of the intersection rectangle\n",
    "    xA = max(bb1[0], bb2[0])\n",
    "    yA = max(bb1[1], bb2[1])\n",
    "    xB = min(bb1[2], bb2[2])\n",
    "    yB = min(bb1[3], bb2[3])\n",
    "\n",
    "  # compute the area of intersection rectangle\n",
    "    interArea = max(xB - xA, 0) * max(yB - yA, 0)\n",
    "    return interArea\n",
    "\n",
    "def improve_bboxes(input_image, positions, bboxes, scores):\n",
    "    size_img_y, size_img_x, _ = input_image.shape\n",
    "    bboxes2 = []\n",
    "    new_boxes = []\n",
    "    new_scores = []\n",
    "    for i, (pos, pos_bbs) in enumerate(zip(positions,bboxes)):\n",
    "        pos_bbs2 = []\n",
    "        pos_x, pos_y = pos\n",
    "        x_max = pos_x+CROP_X\n",
    "        y_max = pos_y+CROP_Y\n",
    "        if x_max>=size_img_x: \n",
    "            x_max = size_img_x\n",
    "        if y_max>=size_img_y:\n",
    "            y_max = size_img_y\n",
    "        for j, bb in enumerate(pos_bbs):\n",
    "            bbx_min, bby_min, bbx_max, bby_max = bb\n",
    "            if not (((bbx_min <= pos_x + 10) and (bbx_min != 0)) or ((bby_min <= pos_y +10) and (bby_min != 0)) or\n",
    "              ((bbx_max >= x_max-10) and (bbx_max != size_img_x)) or ((bby_max >= y_max-10) and (bby_max != size_img_y))):\n",
    "                pos_bbs2.append(bb)\n",
    "                new_scores.append(scores[i][j])\n",
    "            else:\n",
    "                center_x = (bbx_min + bbx_max) // 2\n",
    "                center_y = (bby_min + bby_max) // 2\n",
    "                new_x_min = center_x - (CROP_X // 2)\n",
    "                new_x_max = center_x + (CROP_X // 2)\n",
    "                new_y_min = center_y - (CROP_Y // 2)\n",
    "                new_y_max = center_y + (CROP_Y // 2)\n",
    "                if new_x_min<=0:\n",
    "                    new_x_min = 0\n",
    "                if new_y_min <= 0:\n",
    "                    new_y_min = 0\n",
    "                if new_x_max>=size_img_x:\n",
    "                    new_x_max = size_img_x\n",
    "                if new_y_max>=size_img_y:\n",
    "                    new_y_max = size_img_y\n",
    "                new_crop = input_image[new_y_min:new_y_max,new_x_min:new_x_max]\n",
    "                new_boxes_predict, new_scores_predict = predict_small_image(new_crop, model)\n",
    "\n",
    "                for box, new_score in zip(new_boxes_predict, new_scores_predict):\n",
    "                    new_box = (box[0]+new_x_min,box[1]+new_y_min,box[2]+new_x_min,box[3]+new_y_min)\n",
    "                    interArea = bounding_boxes_intersection_area(new_box,bb)\n",
    "                    bbArea = abs((bb[2] - bb[0]) * (bb[3] - bb[1]))\n",
    "                    new_box_add = False\n",
    "                    if (interArea >= bbArea*0.5):\n",
    "                        new_box_add = True\n",
    "                        for bb2 in new_boxes:\n",
    "                            interArea2 = bounding_boxes_intersection_area(bb2,new_box)\n",
    "                            bb2Area = abs((bb2[2] - bb2[0]) * (bb2[3] - bb2[1]))\n",
    "                            if (interArea2 >= 0.5*bb2Area):\n",
    "                                new_box_add = False\n",
    "\n",
    "                    if new_box_add:\n",
    "                        pos_bbs2.append(new_box)\n",
    "                        new_boxes.append(new_box)\n",
    "                        new_scores.append(new_score)\n",
    "\n",
    "        bboxes2.append(pos_bbs2)\n",
    "    return bboxes2, new_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc131e2c-6af0-49b0-985c-12393e7df762",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics.detection import MeanAveragePrecision\n",
    "\n",
    "def compute_metric(images):\n",
    "    all_images_preds_new = []\n",
    "    all_images_preds = []\n",
    "\n",
    "    all_targets=[]\n",
    "    metric_new_boxes = MeanAveragePrecision(iou_type=\"bbox\", iou_thresholds=[0.50])\n",
    "    metric_old_boxes = MeanAveragePrecision(iou_type=\"bbox\", iou_thresholds=[0.50])\n",
    "\n",
    "    for image in images:\n",
    "        input_image_name = image\n",
    "        positions, bboxes, scores = predict_image(input_image_name, model)\n",
    "        input_image = cv2.imread(input_image_name)\n",
    "        input_image = input_image.astype(np.float32)\n",
    "        input_image /= 255.0\n",
    "        input_image = np.array(input_image)        \n",
    "        # Boxes without improvement\n",
    "        bboxes_old = [item for sublist in bboxes for item in sublist]\n",
    "        scores_old = [item for sublist in scores if sublist.size!=0 for item in sublist]    \n",
    "        all_images_preds.append(dict(boxes=torch.tensor(bboxes_old), scores=torch.tensor(scores_old), labels=torch.tensor([1]*len(bboxes_old))))\n",
    "\n",
    "        # Boxes after the refinement step\n",
    "        bboxes_new, scores_new = improve_bboxes(input_image, positions, bboxes, scores)\n",
    "        bboxes_new = [item for sublist in bboxes_new for item in sublist]\n",
    "        all_images_preds_new.append(dict(boxes=torch.tensor(bboxes_new), scores=torch.tensor(scores_new), labels=torch.tensor([1]*len(bboxes_new))))\n",
    "\n",
    "        # Original boxes\n",
    "        input_labels_name = BOXES_FOLDER + Path(input_image_name).stem + \".xml\"\n",
    "        a = parse_annotation(input_labels_name)\n",
    "        all_targets.append(dict(boxes=torch.tensor(a['boxes']),labels=torch.tensor(a['labels'])))\n",
    "\n",
    "    metric_new_boxes.update(all_images_preds_new, all_targets)\n",
    "    metric_new_boxes = metric_new_boxes.compute()\n",
    "    metric_old_boxes.update(all_images_preds, all_targets)\n",
    "    metric_old_boxes = metric_old_boxes.compute()\n",
    "    return metric_new_boxes, metric_old_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a30761b-40bc-41a2-90f6-6d43d47f6582",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('df_dataset.csv')\n",
    "df_posic = df[['file_image', 'name', 'file_boxes', 'pos_img', 'is_train', 'size_img_x', 'size_img_y']].drop_duplicates().sort_values('pos_img').reset_index(drop=True)\n",
    "lista_names_imagenes = df_posic['file_image'].values\n",
    "numero_imagen = dict(zip(lista_names_imagenes, np.arange(len(lista_names_imagenes))))\n",
    "df_posic['pos_img'] = df_posic['file_image'].map(numero_imagen)\n",
    "df['pos_img'] = df['file_image'].map(numero_imagen)\n",
    "\n",
    "input_images_names = list(df_posic.loc[df_posic['is_train']==False]['file_image'])\n",
    "if DEBLURGAN:\n",
    "    input_images_names = [\"dataset/images_deblurGAN/\"+Path(file_image).stem +\".jpg\" for file_image in input_images_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fba39a93-6f42-4e35-b927-1ccf5f30dce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 313.0843665599823 seconds\n",
      "Without prediction refinement: map50 = tensor(0.7764) mar100 = tensor(0.8476)\n",
      "With prediction refinement: map50 = tensor(0.9073) mar100 = tensor(0.9703)\n"
     ]
    }
   ],
   "source": [
    "init = time.time()\n",
    "metric_new, metric_old = compute_metric(input_images_names)    \n",
    "final_time = time.time() - init\n",
    "print(\"Time:\", final_time, \"seconds\")\n",
    "print(\"Without prediction refinement: map50 =\", metric_old[\"map_50\"], \"mar100 =\", metric_old[\"mar_100\"])\n",
    "print(\"With prediction refinement: map50 =\", metric_new[\"map_50\"], \"mar100 =\", metric_new[\"mar_100\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e43d63-f1d3-4073-923c-ff8c370509aa",
   "metadata": {},
   "source": [
    "**0096_resnet18_fpn_th50**\n",
    "- Without prediction refinement: map50 = tensor(0.7797) mar100 = tensor(0.8476)\n",
    "- With prediction refinement: map50 = tensor(0.8494) mar100 = tensor(0.9480)\n",
    "\n",
    "**0105_resnet18_fpn_th50_deblurGAN**\n",
    "- Time: 224.897 seconds\n",
    "- Without prediction refinement: map50 = tensor(0.7752) mar100 = tensor(0.8513)\n",
    "- With prediction refinement: map50 = tensor(0.8832) mar100 = tensor(0.9554)\n",
    "\n",
    "**0070_fasterrcnn_resnet50_fpn_th50**\n",
    "- Without prediction refinement: map50 = tensor(0.7765) mar100 = tensor(0.8439)\n",
    "- With prediction refinement: map50 = tensor(0.8578) mar100 = tensor(0.9628)\n",
    "\n",
    "**0094_fasterrcnn_resnet50_fpn_deblurGAN**\n",
    "- Time: 316.895 seconds\n",
    "- Without prediction refinement: map50 = tensor(0.7798) mar100 = tensor(0.8439)\n",
    "- With prediction refinement: map50 = tensor(0.9073) mar100 = tensor(0.9665)\n",
    "\n",
    "**0103_resnet152_fpn_th50**\n",
    "- Time: 637.326 seconds\n",
    "- Without prediction refinement: map50 = tensor(0.7939) mar100 = tensor(0.8513)\n",
    "- With prediction refinement: map50 = tensor(0.8944) mar100 = tensor(0.9628)\n",
    "\n",
    "**0104_resnet152_fpn_th50_deblurGAN**\n",
    "- Time: 595.263 seconds\n",
    "- Without prediction refinement: map50 = tensor(0.7086) mar100 = tensor(0.8067)\n",
    "- With prediction refinement: map50 = tensor(0.8514) mar100 = tensor(0.9405)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
